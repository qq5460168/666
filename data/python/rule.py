#!/usr/bin/env python3

import os
import re
import requests
import json
import logging
from datetime import datetime
from typing import List, Tuple, Dict

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# é…ç½®å‚æ•°
RULE_SOURCES_FILE = 'sources.txt'         # è§„åˆ™æ¥æºæ–‡ä»¶
OUTPUT_FILE = 'merged-filter.txt'        # è¾“å‡ºæ–‡ä»¶
STATS_FILE = 'rule_stats.json'           # ç»Ÿè®¡æ–‡ä»¶
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'

# æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—åŒ–
REGEX_PATTERNS = {
    "comment": re.compile(r'^[!#]'),      # æ³¨é‡Šè¡Œ
    "blank": re.compile(r'^\s*$'),       # ç©ºè¡Œ
    "domain": re.compile(r'^(@@)?(\|\|)?([a-zA-Z0-9-*_.]+)(\^|\$|/)?'),
    "element": re.compile(r'##.+'),      # å…ƒç´ è§„åˆ™
    "regex_rule": re.compile(r'^/.*/$'), # æ­£åˆ™è§„åˆ™
    "modifier": re.compile(r'\$(~?[\w-]+(=[^,\s]+)?(,~?[\w-]+(=[^,\s]+)?)*)$')
}

def is_valid_rule(line: str) -> bool:
    """
    éªŒè¯è§„åˆ™æœ‰æ•ˆæ€§
    :param line: è§„åˆ™è¡Œ
    :return: æ˜¯å¦æœ‰æ•ˆ
    """
    # å¦‚æœæ˜¯æ³¨é‡Šè¡Œæˆ–ç©ºè¡Œï¼Œç›´æ¥è¿”å› False
    if REGEX_PATTERNS["comment"].match(line) or REGEX_PATTERNS["blank"].match(line):
        return False
    # éªŒè¯è§„åˆ™æ˜¯å¦åŒ¹é…æœ‰æ•ˆæ ¼å¼
    return any([
        REGEX_PATTERNS["domain"].match(line),
        REGEX_PATTERNS["element"].search(line),
        REGEX_PATTERNS["regex_rule"].match(line),
        REGEX_PATTERNS["modifier"].search(line)
    ])

def download_rules(url: str) -> Tuple[List[str], List[str]]:
    """
    ä¸‹è½½è§„åˆ™å¹¶éªŒè¯
    :param url: è§„åˆ™æ¥æº URL æˆ–æœ¬åœ°æ–‡ä»¶è·¯å¾„
    :return: (æœ‰æ•ˆè§„åˆ™åˆ—è¡¨, æ— æ•ˆè§„åˆ™åˆ—è¡¨)
    """
    valid_rules = []
    invalid_rules = []
    try:
        if url.startswith('file:'):
            # è¯»å–æœ¬åœ°æ–‡ä»¶
            file_path = url.split('file:')[1].strip()
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = (line.strip() for line in f)
        else:
            # ä¸‹è½½è¿œç¨‹æ–‡ä»¶
            resp = requests.get(url, headers={'User-Agent': USER_AGENT}, timeout=15)
            resp.raise_for_status()
            lines = (line.strip() for line in resp.text.splitlines())

        for line in lines:
            if is_valid_rule(line):
                valid_rules.append(line)
            elif line:
                invalid_rules.append(line)

    except requests.exceptions.RequestException as e:
        logging.error(f"âš ï¸ ä¸‹è½½å¤±è´¥: {url} - {str(e)}")
    except FileNotFoundError:
        logging.error(f"âš ï¸ æœ¬åœ°æ–‡ä»¶æœªæ‰¾åˆ°: {url}")
    except Exception as e:
        logging.error(f"âš ï¸ æœªçŸ¥é”™è¯¯: {url} - {str(e)}")

    return valid_rules, invalid_rules

def write_stats(rule_count: int) -> None:
    """
    å†™å…¥è§„åˆ™ç»Ÿè®¡ä¿¡æ¯åˆ° JSON æ–‡ä»¶
    :param rule_count: æœ‰æ•ˆè§„åˆ™æ•°
    """
    stats = {
        "rule_count": rule_count,
        "last_update": datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')
    }
    try:
        with open(STATS_FILE, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=4)
        logging.info(f"âœ… å·²æ›´æ–°ç»Ÿè®¡ä¿¡æ¯: {STATS_FILE}")
    except Exception as e:
        logging.error(f"å†™å…¥ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

def process_sources(sources: List[str]) -> Tuple[set, Dict[str, List[str]]]:
    """
    å¤„ç†è§„åˆ™æ¥æº
    :param sources: è§„åˆ™æ¥æºåˆ—è¡¨
    :return: åˆå¹¶åçš„è§„åˆ™é›†åˆå’Œé”™è¯¯æŠ¥å‘Š
    """
    merged_rules = set()
    error_reports = {}

    for url in sources:
        logging.info(f"ğŸ“¥ æ­£åœ¨å¤„ç†: {url}")
        valid_rules, invalid_rules = download_rules(url)
        merged_rules.update(valid_rules)

        if invalid_rules:
            error_reports[url] = invalid_rules
            logging.warning(f"âš ï¸ å‘ç° {len(invalid_rules)} æ¡æ— æ•ˆè§„åˆ™")

    return merged_rules, error_reports

def save_merged_rules(rules: set, output_file: str) -> None:
    """
    ä¿å­˜åˆå¹¶åçš„è§„åˆ™åˆ°æ–‡ä»¶
    :param rules: åˆå¹¶åçš„è§„åˆ™é›†åˆ
    :param output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    try:
        # æŒ‰ä¼˜å…ˆçº§æ’åºè§„åˆ™
        sorted_rules = sorted(rules, key=lambda x: (
            not x.startswith('||'),   # ä¼˜å…ˆåŸŸåè§„åˆ™
            not x.startswith('##'),   # å…¶æ¬¡æ˜¯å…ƒç´ è§„åˆ™
            x                         # æœ€åæŒ‰å­—å…¸é¡ºåºæ’åº
        ))
        # å†™å…¥æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(sorted_rules))
        logging.info(f"âœ… è§„åˆ™åˆå¹¶å®Œæˆï¼Œè¾“å‡ºåˆ° {output_file}")
    except Exception as e:
        logging.error(f"å†™å…¥åˆå¹¶è§„åˆ™æ–‡ä»¶å¤±è´¥: {e}")

def main() -> None:
    """
    ä¸»å‡½æ•°ï¼šå¤„ç†è§„åˆ™åˆå¹¶ã€éªŒè¯å’Œç»Ÿè®¡
    """
    logging.info("ğŸ“‚ å¼€å§‹å¤„ç†è§„åˆ™æ–‡ä»¶")

    # æ£€æŸ¥è§„åˆ™æ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(RULE_SOURCES_FILE):
        logging.error(f"è§„åˆ™æ¥æºæ–‡ä»¶ {RULE_SOURCES_FILE} æœªæ‰¾åˆ°ï¼")
        return

    try:
        # è¯»å–è§„åˆ™æ¥æº
        with open(RULE_SOURCES_FILE, 'r', encoding='utf-8') as f:
            sources = [line.strip() for line in f if line.strip()]
    except Exception as e:
        logging.error(f"è¯»å–è§„åˆ™æ¥æºæ–‡ä»¶å¤±è´¥: {e}")
        return

    # å¤„ç†è§„åˆ™æ¥æº
    merged_rules, error_reports = process_sources(sources)

    # ä¿å­˜åˆå¹¶åçš„è§„åˆ™
    save_merged_rules(merged_rules, OUTPUT_FILE)

    # å†™å…¥ç»Ÿè®¡ä¿¡æ¯
    write_stats(len(merged_rules))

    # è¾“å‡ºé”™è¯¯æŠ¥å‘Š
    if error_reports:
        logging.warning("\nâš ï¸ ä»¥ä¸‹æ¥æºå­˜åœ¨æ— æ•ˆè§„åˆ™:")
        for url, errors in error_reports.items():
            logging.warning(f"  - æ¥æº: {url}")
            for error in errors:
                logging.warning(f"    - æ— æ•ˆè§„åˆ™: {error}")

if __name__ == "__main__":
    main()